#!/usr/bin/env python3
"""
Pol√≠ticas customizadas para agentes MARL no sistema de demand response.

Este m√≥dulo implementa pol√≠ticas customizadas para Stable Baselines3,
otimizadas para o ambiente CityLearn e cen√°rios multi-agente cooperativos.
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Any, Optional, Union
import warnings
warnings.filterwarnings('ignore')

from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.utils import get_device
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor


class MultiAgentFeaturesExtractor(BaseFeaturesExtractor):
    """
    Extrator de features customizado para m√∫ltiplos agentes.

    Este extrator processa observa√ß√µes de m√∫ltiplos agentes e opcionalmente
    incorpora informa√ß√µes de comunica√ß√£o entre eles.
    """

    def __init__(self, observation_space, num_agents: int = 5, communication_dim: int = 0):
        """
        Inicializa o extrator de features.

        Args:
            observation_space: Espa√ßo de observa√ß√£o
            num_agents: N√∫mero de agentes
            communication_dim: Dimens√£o do canal de comunica√ß√£o
        """
        super().__init__(observation_space, features_dim=256)

        self.num_agents = num_agents
        self.communication_dim = communication_dim

        # Rede para processar observa√ß√µes individuais
        self.agent_network = nn.Sequential(
            nn.Linear(28, 64),  # 28 features por agente (CityLearn)
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )

        # Rede para processar comunica√ß√£o
        if communication_dim > 0:
            self.comm_network = nn.Sequential(
                nn.Linear(communication_dim, 32),
                nn.ReLU(),
                nn.Linear(32, 16)
            )

        # Rede para combinar features
        input_dim = 32 * num_agents + (16 if communication_dim > 0 else 0)
        self.combination_network = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 160),  # Ajustado para 160 para compatibilidade com action_space
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        Forward pass do extrator.

        Args:
            observations: Tensor de observa√ß√µes

        Returns:
            torch.Tensor: Features extra√≠das
        """
        batch_size = observations.shape[0]
        features_list = []

        # Processar cada agente
        for i in range(self.num_agents):
            agent_obs = observations[:, i*28:(i+1)*28]  # 28 features por agente
            agent_features = self.agent_network(agent_obs)
            features_list.append(agent_features)

        # Concatenar features de todos os agentes
        all_agent_features = torch.cat(features_list, dim=1)

        # Processar comunica√ß√£o se dispon√≠vel
        if self.communication_dim > 0:
            # Assumir que comunica√ß√£o est√° nas √∫ltimas communication_dim features
            comm_features = observations[:, -self.communication_dim:]
            comm_processed = self.comm_network(comm_features)
            combined_features = torch.cat([all_agent_features, comm_processed], dim=1)
        else:
            combined_features = all_agent_features

        # Combinar todas as features
        final_features = self.combination_network(combined_features)

        return final_features


class MultiAgentPolicy(BasePolicy):
    """
    Pol√≠tica customizada para m√∫ltiplos agentes com comunica√ß√£o opcional.

    Esta pol√≠tica implementa uma rede neural que processa observa√ß√µes de
    m√∫ltiplos agentes e opcionalmente incorpora comunica√ß√£o entre eles.
    """

    def __init__(self, observation_space, action_space, num_agents: int = 5,
                 communication_dim: int = 0, shared_parameters: bool = True):
        """
        Inicializa a pol√≠tica multi-agente.

        Args:
            observation_space: Espa√ßo de observa√ß√£o
            action_space: Espa√ßo de a√ß√£o
            num_agents: N√∫mero de agentes
            communication_dim: Dimens√£o do canal de comunica√ß√£o
            shared_parameters: Se deve compartilhar par√¢metros entre agentes
        """
        super().__init__(observation_space, action_space)

        self.num_agents = num_agents
        self.communication_dim = communication_dim
        self.shared_parameters = shared_parameters

        # Features extractor
        self.features_extractor = MultiAgentFeaturesExtractor(
            observation_space, num_agents, communication_dim
        )

        # Rede neural compartilhada
        self.shared_network = nn.Sequential(
            nn.Linear(160, 256),  # Ajustado para 160 entradas
            nn.ReLU(),
            nn.Linear(256, 160),
            nn.ReLU()
        )

        # Processamento de comunica√ß√£o
        if communication_dim > 0:
            self.comm_network = nn.Sequential(
                nn.Linear(communication_dim, 64),
                nn.ReLU(),
                nn.Linear(64, 32)
            )

        # Cabe√ßa da pol√≠tica
        input_dim = 160  # Compat√≠vel com o ambiente CityLearn (5 a√ß√µes)
        self.action_network = nn.Linear(input_dim, action_space.shape[0])

        # Inicializa√ß√£o dos pesos
        self.apply(self._init_weights)

    def _init_weights(self, module: nn.Module):
        """Inicializa√ß√£o customizada dos pesos."""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            nn.init.constant_(module.bias, 0.0)

    def forward(self, obs: torch.Tensor, communication: Optional[torch.Tensor] = None):
        """
        Forward pass da pol√≠tica.

        Args:
            obs: Observa√ß√µes do ambiente
            communication: Tensor de comunica√ß√£o (opcional)

        Returns:
            torch.Tensor: A√ß√µes
        """
        # Extrair features
        features = self.features_extractor(obs)

        # Processar features compartilhadas
        shared_features = self.shared_network(features)

        # Processar comunica√ß√£o se dispon√≠vel
        if communication is not None and self.communication_dim > 0:
            comm_features = self.comm_network(communication)
            combined = torch.cat([shared_features, comm_features], dim=-1)
        else:
            combined = shared_features

        # Gerar a√ß√£o
        action = self.action_network(combined)
        return action

    def _predict(self, observation: torch.Tensor, deterministic: bool = True):
        """
        Prediz a√ß√£o (m√©todo interno do SB3).

        Args:
            observation: Observa√ß√£o do ambiente
            deterministic: Se deve usar modo determin√≠stico

        Returns:
            torch.Tensor: A√ß√£o predita
        """
        return self.forward(observation, None)

    def predict(self, observation: np.ndarray, communication: Optional[np.ndarray] = None,
                deterministic: bool = True):
        """
        Prediz a√ß√£o baseada na observa√ß√£o.

        Args:
            observation: Observa√ß√£o do ambiente
            communication: Estado de comunica√ß√£o (opcional)
            deterministic: Se deve usar modo determin√≠stico

        Returns:
            Tuple: (a√ß√£o, estado)
        """
        self.set_training_mode(False)

        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)

        if communication is not None:
            comm_tensor = torch.tensor(communication, dtype=torch.float32).unsqueeze(0)
        else:
            comm_tensor = None

        with torch.no_grad():
            action = self.forward(obs_tensor, comm_tensor)

        action = action.squeeze(0).cpu().numpy()

        if deterministic:
            return action, None
        else:
            # Adicionar ru√≠do para explora√ß√£o
            noise = np.random.normal(0, 0.1, size=action.shape)
            return action + noise, None


class CooperativePolicy(MultiAgentPolicy):
    """
    Pol√≠tica otimizada para coopera√ß√£o entre agentes.

    Esta pol√≠tica estende a MultiAgentPolicy com mecanismos espec√≠ficos
    para incentivar a coopera√ß√£o entre os agentes.
    """

    def __init__(self, observation_space, action_space, num_agents: int = 5,
                 communication_dim: int = 32, cooperation_strength: float = 0.1):
        """
        Inicializa a pol√≠tica cooperativa.

        Args:
            observation_space: Espa√ßo de observa√ß√£o
            action_space: Espa√ßo de a√ß√£o
            num_agents: N√∫mero de agentes
            communication_dim: Dimens√£o do canal de comunica√ß√£o
            cooperation_strength: For√ßa da coopera√ß√£o (0.0 a 1.0)
        """
        super().__init__(observation_space, action_space, num_agents, communication_dim)

        self.cooperation_strength = cooperation_strength

        # Camada adicional para processamento cooperativo
        self.cooperation_network = nn.Sequential(
            nn.Linear(128 + 32, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.Tanh()  # Limitar valores entre -1 e 1
        )

        # Mecanismo de aten√ß√£o para comunica√ß√£o
        self.attention = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)

    def forward(self, obs: torch.Tensor, communication: Optional[torch.Tensor] = None):
        """Forward pass com mecanismos cooperativos."""
        # Processamento base
        features = self.features_extractor(obs)
        shared_features = self.shared_network(features)

        # Processar comunica√ß√£o com aten√ß√£o
        if communication is not None and self.communication_dim > 0:
            comm_features = self.comm_network(communication)

            # Aplicar aten√ß√£o aos features de comunica√ß√£o
            attention_output, _ = self.attention(comm_features.unsqueeze(0),
                                               comm_features.unsqueeze(0),
                                               comm_features.unsqueeze(0))
            attention_output = attention_output.squeeze(0)

            # Combinar com coopera√ß√£o
            combined = torch.cat([shared_features, attention_output], dim=-1)
            coop_features = self.cooperation_network(combined)

            # Aplicar for√ßa de coopera√ß√£o
            final_features = shared_features + self.cooperation_strength * coop_features
        else:
            final_features = shared_features

        # Gerar a√ß√£o
        action = self.action_network(final_features)
        return action


class CentralizedPolicy(BasePolicy):
    """
    Pol√≠tica centralizada para controle global de todos os pr√©dios.

    Esta pol√≠tica implementa uma rede neural que processa o estado global
    de todos os pr√©dios e gera a√ß√µes para todos simultaneamente.
    """

    def __init__(self, observation_space, action_space, num_buildings: int = 5):
        """
        Inicializa a pol√≠tica centralizada.

        Args:
            observation_space: Espa√ßo de observa√ß√£o global
            action_space: Espa√ßo de a√ß√£o global
            num_buildings: N√∫mero de pr√©dios controlados
        """
        super().__init__(observation_space, action_space)

        self.num_buildings = num_buildings

        # Rede neural para controle centralizado
        self.global_network = nn.Sequential(
            nn.Linear(observation_space.shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 160),  # Ajustado para 160 para compatibilidade
            nn.ReLU()
        )

        # Cabe√ßas separadas para cada pr√©dio
        self.building_heads = nn.ModuleList([
            nn.Linear(160, 1) for _ in range(num_buildings)
        ])

        # Inicializa√ß√£o
        self.apply(self._init_weights)

    def _init_weights(self, module: nn.Module):
        """Inicializa√ß√£o dos pesos."""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            nn.init.constant_(module.bias, 0.0)

    def forward(self, obs: torch.Tensor):
        """Forward pass para controle centralizado."""
        # Processar estado global
        global_features = self.global_network(obs)

        # Gerar a√ß√µes para cada pr√©dio
        actions = []
        for head in self.building_heads:
            action = head(global_features)
            actions.append(action)

        return torch.cat(actions, dim=-1)

    def _predict(self, observation: torch.Tensor, deterministic: bool = True):
        """
        Prediz a√ß√£o (m√©todo interno do SB3).

        Args:
            observation: Observa√ß√£o do ambiente
            deterministic: Se deve usar modo determin√≠stico

        Returns:
            torch.Tensor: A√ß√£o predita
        """
        return self.forward(observation)

    def predict(self, observation: np.ndarray, deterministic: bool = True):
        """Prediz a√ß√µes para todos os pr√©dios."""
        self.set_training_mode(False)

        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)

        with torch.no_grad():
            actions = self.forward(obs_tensor)

        actions = actions.squeeze(0).cpu().numpy()

        if deterministic:
            return actions, None
        else:
            # Adicionar ru√≠do para explora√ß√£o
            noise = np.random.normal(0, 0.1, size=actions.shape)
            return actions + noise, None


class AttentionPolicy(BasePolicy):
    """
    Pol√≠tica baseada em mecanismos de aten√ß√£o para comunica√ß√£o.

    Esta pol√≠tica usa transformers/attention para processar comunica√ß√£o
    entre agentes de forma mais sofisticada.
    """

    def __init__(self, observation_space, action_space, num_agents: int = 5,
                 communication_dim: int = 32, attention_heads: int = 4):
        """
        Inicializa a pol√≠tica com aten√ß√£o.

        Args:
            observation_space: Espa√ßo de observa√ß√£o
            action_space: Espa√ßo de a√ß√£o
            num_agents: N√∫mero de agentes
            communication_dim: Dimens√£o do canal de comunica√ß√£o
            attention_heads: N√∫mero de cabe√ßas de aten√ß√£o
        """
        super().__init__(observation_space, action_space)

        self.num_agents = num_agents
        self.communication_dim = communication_dim
        self.attention_heads = attention_heads

        # Features extractor
        self.features_extractor = MultiAgentFeaturesExtractor(
            observation_space, num_agents, communication_dim
        )

        # Mecanismo de aten√ß√£o para comunica√ß√£o
        self.attention = nn.MultiheadAttention(
            embed_dim=128,
            num_heads=attention_heads,
            batch_first=True
        )

        # Rede para processar sa√≠da da aten√ß√£o
        self.attention_network = nn.Sequential(
            nn.Linear(160, 80),  # Ajustado para compatibilidade
            nn.ReLU(),
            nn.Linear(80, 40)
        )

        # Cabe√ßa da pol√≠tica
        self.action_network = nn.Linear(160 + 40, action_space.shape[0])

        # Inicializa√ß√£o
        self.apply(self._init_weights)

    def _init_weights(self, module: nn.Module):
        """Inicializa√ß√£o dos pesos."""
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            nn.init.constant_(module.bias, 0.0)

    def _predict(self, observation: torch.Tensor, deterministic: bool = True):
        """
        Prediz a√ß√£o (m√©todo interno do SB3).

        Args:
            observation: Observa√ß√£o do ambiente
            deterministic: Se deve usar modo determin√≠stico

        Returns:
            torch.Tensor: A√ß√£o predita
        """
        return self.forward(observation, None)

    def forward(self, obs: torch.Tensor, communication: Optional[torch.Tensor] = None):
        """Forward pass com aten√ß√£o."""
        # Extrair features
        features = self.features_extractor(obs)

        # Aplicar aten√ß√£o se comunica√ß√£o dispon√≠vel
        if communication is not None and self.communication_dim > 0:
            # Usar features como query, key e value para aten√ß√£o
            attention_output, _ = self.attention(features.unsqueeze(0),
                                                features.unsqueeze(0),
                                                features.unsqueeze(0))
            attention_output = attention_output.squeeze(0)

            # Processar sa√≠da da aten√ß√£o
            attention_features = self.attention_network(attention_output)

            # Combinar features originais com aten√ß√£o
            combined = torch.cat([features, attention_features], dim=-1)
        else:
            combined = features

        # Gerar a√ß√£o
        action = self.action_network(combined)
        return action


# Registrar pol√≠ticas no Stable Baselines3
def register_custom_policies():
    """Registra pol√≠ticas customizadas no SB3."""
    try:
        # Nota: No SB3, pol√≠ticas customizadas s√£o registradas automaticamente
        # quando importadas. N√£o h√° necessidade de register_policy.
        print("‚úÖ Pol√≠ticas customizadas dispon√≠veis:")
        print("   - MultiAgentPolicy")
        print("   - CooperativePolicy")
        print("   - CentralizedPolicy")
        print("   - AttentionPolicy")
        print("   (Pol√≠ticas podem ser usadas diretamente com policy='MultiAgentPolicy')")

    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao configurar pol√≠ticas: {e}")


# Fun√ß√µes utilit√°rias para cria√ß√£o de pol√≠ticas
def create_policy_from_config(policy_name: str, observation_space, action_space, config: Dict):
    """
    Cria pol√≠tica baseada na configura√ß√£o.

    Args:
        policy_name: Nome da pol√≠tica
        observation_space: Espa√ßo de observa√ß√£o
        action_space: Espa√ßo de a√ß√£o
        config: Configura√ß√µes da pol√≠tica

    Returns:
        Pol√≠tica configurada
    """
    if policy_name == "MultiAgentPolicy":
        return MultiAgentPolicy(
            observation_space=observation_space,
            action_space=action_space,
            num_agents=config.get("num_agents", 5),
            communication_dim=config.get("communication_dim", 0),
            shared_parameters=config.get("shared_parameters", True)
        )
    elif policy_name == "CooperativePolicy":
        return CooperativePolicy(
            observation_space=observation_space,
            action_space=action_space,
            num_agents=config.get("num_agents", 5),
            communication_dim=config.get("communication_dim", 32),
            cooperation_strength=config.get("cooperation_strength", 0.1)
        )
    elif policy_name == "CentralizedPolicy":
        return CentralizedPolicy(
            observation_space=observation_space,
            action_space=action_space,
            num_buildings=config.get("num_buildings", 5)
        )
    elif policy_name == "AttentionPolicy":
        return AttentionPolicy(
            observation_space=observation_space,
            action_space=action_space,
            num_agents=config.get("num_agents", 5),
            communication_dim=config.get("communication_dim", 32),
            attention_heads=config.get("attention_heads", 4)
        )
    else:
        raise ValueError(f"Pol√≠tica n√£o suportada: {policy_name}")


def get_policy_info(policy) -> Dict:
    """
    Retorna informa√ß√µes sobre uma pol√≠tica.

    Args:
        policy: Pol√≠tica a analisar

    Returns:
        Dict: Informa√ß√µes da pol√≠tica
    """
    info = {
        "policy_type": type(policy).__name__,
        "parameters": 0,
        "layers": []
    }

    # Contar par√¢metros
    if hasattr(policy, 'parameters'):
        info["parameters"] = sum(p.numel() for p in policy.parameters())

    # Descrever arquitetura
    if hasattr(policy, 'shared_network'):
        info["layers"].append("shared_network")
    if hasattr(policy, 'comm_network'):
        info["layers"].append("comm_network")
    if hasattr(policy, 'attention'):
        info["layers"].append("attention")
    if hasattr(policy, 'action_network'):
        info["layers"].append("action_network")

    return info


# Testes das pol√≠ticas
def test_policies():
    """Testa funcionalidades das pol√≠ticas customizadas."""
    print("üß™ Testando pol√≠ticas customizadas...")

    try:
        from src.environment import make_citylearn_vec_env

        # Criar ambiente
        env = make_citylearn_vec_env("citylearn_challenge_2022_phase_1")

        # Testar MultiAgentPolicy
        print("\n1. Testando MultiAgentPolicy...")
        policy = MultiAgentPolicy(
            env.observation_space,
            env.action_space,
            num_agents=5,
            communication_dim=32
        )

        obs, info = env.reset()
        action = policy.predict(obs)[0]

        assert env.action_space.contains(action), "A√ß√£o fora do espa√ßo v√°lido"
        print(f"   ‚úÖ MultiAgentPolicy: {action.shape} a√ß√µes geradas")

        # Testar CentralizedPolicy
        print("\n2. Testando CentralizedPolicy...")
        policy = CentralizedPolicy(
            env.observation_space,
            env.action_space,
            num_buildings=5
        )

        action = policy.predict(obs)[0]
        assert env.action_space.contains(action), "A√ß√£o fora do espa√ßo v√°lido"
        print(f"   ‚úÖ CentralizedPolicy: {action.shape} a√ß√µes geradas")

        # Testar CooperativePolicy
        print("\n3. Testando CooperativePolicy...")
        policy = CooperativePolicy(
            env.observation_space,
            env.action_space,
            num_agents=5,
            communication_dim=32,
            cooperation_strength=0.1
        )

        action = policy.predict(obs)[0]
        assert env.action_space.contains(action), "A√ß√£o fora do espa√ßo v√°lido"
        print(f"   ‚úÖ CooperativePolicy: {action.shape} a√ß√µes geradas")

        env.close()
        print("\n‚úÖ Todas as pol√≠ticas testadas com sucesso!")

        return True

    except Exception as e:
        print(f"‚ùå Erro nos testes: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # Registrar pol√≠ticas e executar testes
    register_custom_policies()
    test_policies()