#!/usr/bin/env python3
"""
Factory functions para cria√ß√£o de agentes MARL.

Este m√≥dulo fornece fun√ß√µes factory para criar diferentes tipos de agentes
de forma padronizada e configur√°vel, facilitando a experimenta√ß√£o e
compara√ß√£o entre diferentes abordagens.
"""

import numpy as np
from typing import Dict, List, Optional, Union, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

from .base_agent import BaseAgent, AgentFactory
from .independent_agent import IndependentAgent, IndependentAgentFactory
from .cooperative_agent import CooperativeAgent, CooperativeAgentFactory
from .centralized_agent import CentralizedAgent, CentralizedAgentFactory


class MultiAgentFactory:
    """
    Factory principal para cria√ß√£o de sistemas multi-agente.

    Esta classe centraliza a cria√ß√£o de diferentes tipos de agentes
    e sistemas multi-agente, permitindo configura√ß√£o flex√≠vel e
    experimenta√ß√£o f√°cil.
    """

    @staticmethod
    def create_agent_system(env, config: Dict) -> Union[BaseAgent, List[BaseAgent]]:
        """
        Cria sistema de agentes baseado na configura√ß√£o.

        Args:
            env: Ambiente de simula√ß√£o
            config: Configura√ß√µes do sistema

        Returns:
            Agente ou lista de agentes
        """
        agent_type = config.get("agent_type", "independent")

        if agent_type == "centralized":
            # Sistema centralizado: um agente controla todos
            return CentralizedAgentFactory.create_centralized_system(env, config)
        else:
            # Sistema multi-agente: m√∫ltiplos agentes
            return AgentFactory.create_multi_agent_system(env, config)

    @staticmethod
    def create_comparison_system(env, agent_types: List[str],
                               configs: Optional[Dict] = None) -> Dict[str, List[BaseAgent]]:
        """
        Cria sistemas de diferentes tipos para compara√ß√£o.

        Args:
            env: Ambiente de simula√ß√£o
            agent_types: Lista de tipos de agentes a criar
            configs: Configura√ß√µes espec√≠ficas por tipo

        Returns:
            Dict: Dicion√°rio com sistemas de agentes por tipo
        """
        systems = {}

        for agent_type in agent_types:
            config = configs.get(agent_type, {}) if configs else {}
            config["agent_type"] = agent_type

            system = MultiAgentFactory.create_agent_system(env, config)
            systems[agent_type] = system

        print(f"‚úÖ Sistemas de compara√ß√£o criados: {list(systems.keys())}")
        return systems

    @staticmethod
    def create_baseline_agents(env) -> Dict[str, List[BaseAgent]]:
        """
        Cria agentes baseline para compara√ß√£o.

        Args:
            env: Ambiente de simula√ß√£o

        Returns:
            Dict: Dicion√°rio com agentes baseline
        """
        baselines = {}

        # Agente aleat√≥rio
        baselines["random"] = RandomAgentFactory.create_multi_agent_system(env)

        # Agente baseado em regras
        baselines["rule_based"] = RuleBasedAgentFactory.create_multi_agent_system(env)

        # Agente independente PPO
        ppo_config = {
            "agent_type": "independent",
            "training": {"algorithm": "PPO", "total_timesteps": 100000}
        }
        baselines["independent_ppo"] = AgentFactory.create_multi_agent_system(env, ppo_config)

        print(f"‚úÖ Agentes baseline criados: {list(baselines.keys())}")
        return baselines


class RandomAgent(BaseAgent):
    """
    Agente que seleciona a√ß√µes aleat√≥rias (baseline).

    Este agente serve como baseline simples para comparar com
    abordagens de aprendizado por refor√ßo.
    """

    def __init__(self, env, agent_id: int, config: Dict):
        """Inicializa agente aleat√≥rio."""
        super().__init__(env, agent_id, config)
        print(f"‚úÖ RandomAgent {agent_id} inicializado")

    def select_action(self, observation: np.ndarray, **kwargs) -> np.ndarray:
        """Seleciona a√ß√£o aleat√≥ria."""
        return self.env.action_space.sample()

    def update_policy(self, experience: Tuple, **kwargs) -> None:
        """N√£o faz nada (agente aleat√≥rio n√£o aprende)."""
        self._log_training_step(experience, loss=0.0)

    def evaluate(self, num_episodes: int = 10) -> Dict:
        """
        Avalia performance do agente aleat√≥rio.

        Args:
            num_episodes: N√∫mero de epis√≥dios para avalia√ß√£o

        Returns:
            Dict: M√©tricas de performance
        """
        print(f"üìä Avaliando RandomAgent {self.agent_id} por {num_episodes} epis√≥dios...")

        episode_rewards = []
        episode_lengths = []

        for episode in range(num_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False

            while not done:
                action = self.select_action(obs)
                obs, reward, done, info = self.env.step(action)

                episode_reward += reward
                episode_length += 1

                # Adicionar logs para debugging
                if episode_length % 1000 == 0:
                    print(f"   - Epis√≥dio {episode+1}: Passo {episode_length}, Done: {done}, Recompensa: {episode_reward:.3f}")

                # Limitar comprimento do epis√≥dio com limite adicional
                if episode_length > 10000:
                    print(f"   - Epis√≥dio {episode+1}: Limite de passos atingido, interrompendo.")
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

        # Calcular estat√≠sticas
        rewards = np.array(episode_rewards)
        lengths = np.array(episode_lengths)

        results = {
            "agent_id": self.agent_id,
            "agent_type": "RandomAgent",
            "num_episodes": num_episodes,
            "mean_reward": np.mean(rewards),
            "std_reward": np.std(rewards),
            "min_reward": np.min(rewards),
            "max_reward": np.max(rewards),
            "mean_length": np.mean(lengths),
            "std_length": np.std(lengths),
            "total_timesteps": self.total_timesteps
        }

        print(f"   - Recompensa m√©dia: {results['mean_reward']:.3f} ¬± {results['std_reward']:.3f}")
        print(f"   - Comprimento m√©dio: {results['mean_length']:.1f} ¬± {results['std_length']:.1f}")

        return results


class RuleBasedAgent(BaseAgent):
    """
    Agente baseado em regras fixas (baseline).

    Este agente implementa regras simples para controle de HVAC
    baseado em thresholds de temperatura e outras heur√≠sticas.
    """

    def __init__(self, env, agent_id: int, config: Dict):
        """
        Inicializa agente baseado em regras.

        Args:
            env: Ambiente de simula√ß√£o
            agent_id: ID do agente
            config: Configura√ß√µes
        """
        super().__init__(env, agent_id, config)

        # Configura√ß√µes das regras
        self.temp_thresholds = config.get("temp_thresholds", {
            "cooling_start": 24.0,
            "cooling_stop": 22.0,
            "heating_start": 20.0,
            "heating_stop": 22.0
        })

        self.hvac_action = 0.0  # Estado atual do HVAC
        self.last_temp = None

        print(f"‚úÖ RuleBasedAgent {agent_id} inicializado")

    def select_action(self, observation: np.ndarray, **kwargs) -> np.ndarray:
        """
        Seleciona a√ß√£o baseada em regras.

        Args:
            observation: Observa√ß√£o do ambiente
            **kwargs: Argumentos adicionais

        Returns:
            np.ndarray: A√ß√£o baseada em regras
        """
        # Extrair temperatura (assumindo que est√° nas primeiras features)
        if len(observation) >= 4:
            current_temp = observation[3]  # Temperatura interna

            # Regra de controle de temperatura
            if current_temp > self.temp_thresholds["cooling_start"]:
                # Iniciar resfriamento
                self.hvac_action = -0.5  # Resfriamento moderado
            elif current_temp < self.temp_thresholds["heating_start"]:
                # Iniciar aquecimento
                self.hvac_action = 0.5   # Aquecimento moderado
            elif (self.temp_thresholds["cooling_stop"] <= current_temp <= self.temp_thresholds["heating_stop"]):
                # Manter temperatura
                self.hvac_action = 0.0

            self.last_temp = current_temp
        else:
            # Fallback para a√ß√£o aleat√≥ria
            self.hvac_action = np.random.uniform(-0.3, 0.3)

        # Adicionar pequeno ru√≠do para explora√ß√£o
        noise = np.random.normal(0, 0.1)
        action = np.array([self.hvac_action + noise])

        return self._validate_action(action)

    def update_policy(self, experience: Tuple, **kwargs) -> None:
        """N√£o faz nada (agente baseado em regras n√£o aprende)."""
        self._log_training_step(experience, loss=0.0)

    def evaluate(self, num_episodes: int = 10) -> Dict:
        """
        Avalia performance do agente baseado em regras.

        Args:
            num_episodes: N√∫mero de epis√≥dios para avalia√ß√£o

        Returns:
            Dict: M√©tricas de performance
        """
        print(f"üìä Avaliando RuleBasedAgent {self.agent_id} por {num_episodes} epis√≥dios...")

        episode_rewards = []
        episode_lengths = []

        for episode in range(num_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False

            while not done:
                action = self.select_action(obs)
                obs, reward, done, info = self.env.step(action)

                episode_reward += reward
                episode_length += 1

                # Adicionar logs para debugging
                if episode_length % 1000 == 0:
                    print(f"   - Epis√≥dio {episode+1}: Passo {episode_length}, Done: {done}, Recompensa: {np.mean(rewards):.3f}")

                # Limitar comprimento do epis√≥dio com limite adicional
                if episode_length > 10000:
                    print(f"   - Epis√≥dio {episode+1}: Limite de passos atingido, interrompendo.")
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

        # Calcular estat√≠sticas
        rewards = np.array(episode_rewards)
        lengths = np.array(episode_lengths)

        results = {
            "agent_id": self.agent_id,
            "agent_type": "RuleBasedAgent",
            "num_episodes": num_episodes,
            "mean_reward": np.mean(rewards),
            "std_reward": np.std(rewards),
            "min_reward": np.min(rewards),
            "max_reward": np.max(rewards),
            "mean_length": np.mean(lengths),
            "std_length": np.std(lengths),
            "total_timesteps": self.total_timesteps
        }

        print(f"   - Recompensa m√©dia: {results['mean_reward']:.3f} ¬± {results['std_reward']:.3f}")
        print(f"   - Comprimento m√©dio: {results['mean_length']:.1f} ¬± {results['std_length']:.1f}")

        return results


class RandomAgentFactory:
    """Factory para agentes aleat√≥rios."""

    @staticmethod
    def create_agent(env, agent_id: int, config: Optional[Dict] = None) -> RandomAgent:
        """Cria agente aleat√≥rio."""
        default_config = {"agent_type": "random"}
        if config:
            default_config.update(config)
        return RandomAgent(env, agent_id, default_config)

    @staticmethod
    def create_multi_agent_system(env, num_agents: Optional[int] = None,
                                config: Optional[Dict] = None) -> List[RandomAgent]:
        """Cria sistema de agentes aleat√≥rios."""
        if num_agents is None:
            num_agents = env.num_buildings

        agents = []
        for i in range(num_agents):
            agent_config = config.copy() if config else {}
            agent_config["agent_id"] = i
            agent = RandomAgentFactory.create_agent(env, i, agent_config)
            agents.append(agent)

        return agents


class RuleBasedAgentFactory:
    """Factory para agentes baseados em regras."""

    @staticmethod
    def create_agent(env, agent_id: int, config: Optional[Dict] = None) -> RuleBasedAgent:
        """Cria agente baseado em regras."""
        default_config = {
            "agent_type": "rule_based",
            "temp_thresholds": {
                "cooling_start": 24.0,
                "cooling_stop": 22.0,
                "heating_start": 20.0,
                "heating_stop": 22.0
            }
        }
        if config:
            default_config.update(config)
        return RuleBasedAgent(env, agent_id, default_config)

    @staticmethod
    def create_multi_agent_system(env, num_agents: Optional[int] = None,
                                config: Optional[Dict] = None) -> List[RuleBasedAgent]:
        """Cria sistema de agentes baseados em regras."""
        if num_agents is None:
            num_agents = env.num_buildings

        agents = []
        for i in range(num_agents):
            agent_config = config.copy() if config else {}
            agent_config["agent_id"] = i
            agent = RuleBasedAgentFactory.create_agent(env, i, agent_config)
            agents.append(agent)

        return agents


def create_agent_from_config(config_path: str, env, agent_id: int = 0):
    """
    Cria agente a partir de arquivo de configura√ß√£o YAML.

    Args:
        config_path: Caminho do arquivo de configura√ß√£o
        env: Ambiente de simula√ß√£o
        agent_id: ID do agente

    Returns:
        Agente configurado
    """
    try:
        import yaml

        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        # Validar configura√ß√£o
        AgentFactory.validate_config(config)

        # Criar agente
        agent_type = config["agents"]["type"]
        agent_config = config["agents"]

        return AgentFactory.create_agent(agent_type, env, agent_id, agent_config)

    except Exception as e:
        print(f"‚ùå Erro ao carregar configura√ß√£o: {e}")
        raise


def create_multi_agent_from_config(config_path: str, env):
    """
    Cria sistema multi-agente a partir de arquivo de configura√ß√£o YAML.

    Args:
        config_path: Caminho do arquivo de configura√ß√£o
        env: Ambiente de simula√ß√£o

    Returns:
        Lista de agentes configurados
    """
    try:
        import yaml

        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        # Validar configura√ß√£o
        AgentFactory.validate_config(config)

        # Criar sistema multi-agente
        return AgentFactory.create_multi_agent_system(env, config)

    except Exception as e:
        print(f"‚ùå Erro ao carregar configura√ß√£o: {e}")
        raise


def get_available_agent_types() -> List[str]:
    """
    Retorna lista de tipos de agentes dispon√≠veis.

    Returns:
        List[str]: Tipos de agentes dispon√≠veis
    """
    return ["independent", "cooperative", "centralized", "random", "rule_based"]


def get_default_config(agent_type: str) -> Dict:
    """
    Retorna configura√ß√£o padr√£o para um tipo de agente.

    Args:
        agent_type: Tipo de agente

    Returns:
        Dict: Configura√ß√£o padr√£o
    """
    configs = {
        "independent": {
            "agent_type": "independent",
            "learning_rate": 3e-4,
            "exploration_rate": 1.0,
            "min_exploration_rate": 0.1,
            "exploration_decay": 0.995,
            "policy_kwargs": {
                "net_arch": [64, 64],
                "activation_fn": "Tanh"
            }
        },
        "cooperative": {
            "agent_type": "cooperative",
            "learning_rate": 3e-4,
            "cooperation_strength": 0.1,
            "shared_policy": True,
            "communication_dim": 32,
            "policy_kwargs": {
                "net_arch": [256, 256, 128],
                "activation_fn": "ReLU"
            }
        },
        "centralized": {
            "agent_type": "centralized",
            "learning_rate": 3e-4,
            "coordination_strategy": "global",
            "policy_kwargs": {
                "net_arch": [512, 256, 128],
                "activation_fn": "ReLU"
            }
        },
        "random": {
            "agent_type": "random"
        },
        "rule_based": {
            "agent_type": "rule_based",
            "temp_thresholds": {
                "cooling_start": 24.0,
                "cooling_stop": 22.0,
                "heating_start": 20.0,
                "heating_stop": 22.0
            }
        }
    }

    return configs.get(agent_type, configs["independent"])


def validate_agent_config(config: Dict) -> bool:
    """
    Valida configura√ß√£o de agente.

    Args:
        config: Configura√ß√£o a validar

    Returns:
        bool: True se v√°lida

    Raises:
        ValueError: Se configura√ß√£o inv√°lida
    """
    required_fields = ["agent_type"]

    for field in required_fields:
        if field not in config:
            raise ValueError(f"Campo obrigat√≥rio ausente: {field}")

    # Validar tipo de agente
    valid_types = get_available_agent_types()
    if config["agent_type"] not in valid_types:
        raise ValueError(f"Tipo de agente inv√°lido: {config['agent_type']}")

    # Validar configura√ß√µes espec√≠ficas
    if config["agent_type"] in ["independent", "cooperative", "centralized"]:
        if "policy_kwargs" not in config:
            config["policy_kwargs"] = {}

        if "net_arch" not in config["policy_kwargs"]:
            config["policy_kwargs"]["net_arch"] = [64, 64]

    return True


# Fun√ß√µes de teste para o factory
def test_agent_factory():
    """Testa funcionalidades do factory."""
    print("üß™ Testando AgentFactory...")

    try:
        from src.environment import make_citylearn_vec_env

        # Criar ambiente
        env = make_citylearn_vec_env("citylearn_challenge_2022_phase_1")

        # Testar cria√ß√£o de agente independente
        print("\n1. Testando IndependentAgent...")
        config = get_default_config("independent")
        agent = AgentFactory.create_agent("independent", env, 0, config)
        print(f"   ‚úÖ Agente criado: {agent}")

        # Testar cria√ß√£o de agente cooperativo
        print("\n2. Testando CooperativeAgent...")
        config = get_default_config("cooperative")
        agent = AgentFactory.create_agent("cooperative", env, 1, config)
        print(f"   ‚úÖ Agente criado: {agent}")

        # Testar cria√ß√£o de agente centralizado
        print("\n3. Testando CentralizedAgent...")
        config = get_default_config("centralized")
        agent = AgentFactory.create_agent("centralized", env, 2, config)
        print(f"   ‚úÖ Agente criado: {agent}")

        # Testar sistema multi-agente
        print("\n4. Testando sistema multi-agente...")
        config = {"agent_type": "independent"}
        agents = AgentFactory.create_multi_agent_system(env, config)
        print(f"   ‚úÖ Sistema criado: {len(agents)} agentes")

        # Testar agentes baseline
        print("\n5. Testando agentes baseline...")
        baselines = MultiAgentFactory.create_baseline_agents(env)
        print(f"   ‚úÖ Baselines criados: {list(baselines.keys())}")

        env.close()
        print("\n‚úÖ AgentFactory testado com sucesso!")
        return True

    except Exception as e:
        print(f"‚ùå Erro no teste do factory: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # Executar testes do factory
    test_agent_factory()