#!/usr/bin/env python3
"""
Agente independente para MARL no sistema de demand response.

Este mÃ³dulo implementa o IndependentAgent que aprende polÃ­ticas individuais
sem considerar ou cooperar com outros agentes. Serve como baseline para
comparaÃ§Ã£o com agentes cooperativos.
"""

import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

from .base_agent import BaseAgent


class IndependentAgent(BaseAgent):
    """
    Agente que aprende independentemente sem cooperaÃ§Ã£o.

    Este agente implementa uma polÃ­tica individual usando Stable Baselines3
    e nÃ£o considera informaÃ§Ãµes de outros agentes. Ã‰ Ãºtil como baseline
    para comparar com abordagens cooperativas.

    Args:
        env: Ambiente de simulaÃ§Ã£o
        agent_id: ID Ãºnico do agente
        config: ConfiguraÃ§Ãµes do agente

    Attributes:
        policy: PolÃ­tica de aprendizado individual
        exploration_rate: Taxa de exploraÃ§Ã£o atual
        learning_rate: Taxa de aprendizado
    """

    def __init__(self, env, agent_id: int, config: Dict):
        """
        Inicializa o agente independente.

        Args:
            env: Ambiente de simulaÃ§Ã£o
            agent_id: ID Ãºnico do agente
            config: ConfiguraÃ§Ãµes do agente
        """
        super().__init__(env, agent_id, config)

        # ConfiguraÃ§Ãµes especÃ­ficas do agente independente
        self.exploration_rate = config.get("exploration_rate", 1.0)
        self.min_exploration_rate = config.get("min_exploration_rate", 0.1)
        self.exploration_decay = config.get("exploration_decay", 0.995)

        # Criar polÃ­tica individual
        self.policy = self._create_policy()

        print(f"âœ… IndependentAgent {agent_id} inicializado com polÃ­tica {type(self.policy).__name__}")

    def _create_policy(self):
        """
        Cria polÃ­tica individual usando Stable Baselines3.

        Returns:
            PolÃ­tica configurada para o agente
        """
        try:
            from stable_baselines3 import PPO

            # ConfiguraÃ§Ãµes da polÃ­tica
            policy_kwargs = self.config.get("policy_kwargs", {})
            net_arch = policy_kwargs.get("net_arch", [64, 64])

            # Usar polÃ­tica padrÃ£o do SB3 para evitar problemas de configuraÃ§Ã£o
            policy = PPO(
                policy="MlpPolicy",
                env=self.env,
                learning_rate=self.learning_rate,
                n_steps=self.config.get("n_steps", 2048),
                batch_size=self.config.get("batch_size", 64),
                n_epochs=self.config.get("n_epochs", 10),
                gamma=self.gamma,
                gae_lambda=self.config.get("gae_lambda", 0.95),
                clip_range=self.config.get("clip_range", 0.2),
                ent_coef=self.entropy_coef,
                vf_coef=self.config.get("vf_coef", 0.5),
                max_grad_norm=self.config.get("max_grad_norm", 0.5),
                verbose=0,
                seed=self.config.get("seed", None)
            )

            return policy

        except ImportError:
            raise ImportError("Stable Baselines3 nÃ£o encontrado. Instale com: pip install stable-baselines3")

    def select_action(self, observation: np.ndarray, **kwargs) -> np.ndarray:
        """
        Seleciona aÃ§Ã£o baseada na observaÃ§Ã£o.

        Args:
            observation: ObservaÃ§Ã£o do ambiente
            **kwargs: Argumentos adicionais (ignorados para agente independente)

        Returns:
            np.ndarray: AÃ§Ã£o selecionada
        """
        if self.policy is None:
            raise RuntimeError("PolÃ­tica nÃ£o inicializada")

        # Predizer aÃ§Ã£o usando a polÃ­tica
        action, _ = self.policy.predict(observation, deterministic=True)

        # Adicionar exploraÃ§Ã£o se configurada
        if self.exploration_rate > 0:
            noise = np.random.normal(0, self.exploration_rate, size=action.shape)
            action = action + noise

        # Validar aÃ§Ã£o
        action = self._validate_action(action)

        return action

    def update_policy(self, experience: Tuple, **kwargs) -> None:
        """
        Atualiza polÃ­tica baseada na experiÃªncia.

        Args:
            experience: Tupla (obs, action, reward, next_obs, done)
            **kwargs: Argumentos adicionais
        """
        if self.policy is None:
            raise RuntimeError("PolÃ­tica nÃ£o inicializada")

        obs, action, reward, next_obs, done = experience

        # Adicionar experiÃªncia ao buffer da polÃ­tica
        if hasattr(self.policy, 'replay_buffer'):
            self.policy.replay_buffer.add(obs, action, reward, next_obs, done)
        else:
            # Para polÃ­ticas on-policy como PPO, adicionar ao rollout buffer
            self.policy.rollout_buffer.add(obs, action, reward, next_obs, done)

        # Atualizar polÃ­tica se necessÃ¡rio
        if hasattr(self.policy, 'collect_rollouts'):
            # PPO e polÃ­ticas similares
            if self.policy.num_timesteps >= self.policy.n_steps:
                self.policy.train()
        else:
            # PolÃ­ticas off-policy
            if len(self.policy.replay_buffer) >= self.policy.batch_size:
                self.policy.train()

        # Atualizar taxa de exploraÃ§Ã£o
        self.exploration_rate = max(
            self.min_exploration_rate,
            self.exploration_rate * self.exploration_decay
        )

        # Log do passo de treinamento
        loss = getattr(self.policy, 'loss', None)
        self._log_training_step(experience, loss)

    def train(self, total_timesteps: int, eval_freq: int = 1000, **kwargs) -> None:
        """
        Treina o agente por um nÃºmero de passos.

        Args:
            total_timesteps: NÃºmero total de passos de treinamento
            eval_freq: FrequÃªncia de avaliaÃ§Ã£o
            **kwargs: Argumentos adicionais para o treinamento
        """
        if self.policy is None:
            raise RuntimeError("PolÃ­tica nÃ£o inicializada")

        print(f"ğŸ‹ï¸ Treinando IndependentAgent {self.agent_id} por {total_timesteps} passos...")

        # Configurar callback para logging
        from stable_baselines3.common.callbacks import BaseCallback

        class TrainingCallback(BaseCallback):
            def __init__(self, agent, eval_freq=1000):
                super().__init__()
                self.agent = agent
                self.eval_freq = eval_freq

            def _on_step(self):
                if self.n_calls % self.eval_freq == 0:
                    stats = self.agent.get_training_stats()
                    print(f"   Passo {self.n_calls}: Recompensa mÃ©dia = {stats['mean_reward']:.3f}")
                return True

        callback = TrainingCallback(self, eval_freq)

        # Treinar polÃ­tica
        self.policy.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )

        print(f"âœ… IndependentAgent {self.agent_id} treinado!")

    def evaluate(self, num_episodes: int = 10) -> Dict:
        """
        Avalia performance do agente.

        Args:
            num_episodes: NÃºmero de episÃ³dios para avaliaÃ§Ã£o

        Returns:
            Dict: MÃ©tricas de performance
        """
        print(f"ğŸ“Š Avaliando IndependentAgent {self.agent_id} por {num_episodes} episÃ³dios...")

        episode_rewards = []
        episode_lengths = []

        for episode in range(num_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_length = 0
            done = False

            while not done:
                action = self.select_action(obs)
                obs, reward, done, info = self.env.step(action)

                episode_reward += reward
                episode_length += 1

                # Limitar comprimento do episÃ³dio para evitar loops infinitos
                if episode_length > 10000:
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

        # Calcular estatÃ­sticas
        rewards = np.array(episode_rewards)
        lengths = np.array(episode_lengths)

        results = {
            "agent_id": self.agent_id,
            "agent_type": "IndependentAgent",
            "num_episodes": num_episodes,
            "mean_reward": np.mean(rewards),
            "std_reward": np.std(rewards),
            "min_reward": np.min(rewards),
            "max_reward": np.max(rewards),
            "mean_length": np.mean(lengths),
            "std_length": np.std(lengths),
            "total_timesteps": self.total_timesteps
        }

        print(f"   - Recompensa mÃ©dia: {results['mean_reward']:.3f} Â± {results['std_reward']:.3f}")
        print(f"   - Comprimento mÃ©dio: {results['mean_length']:.1f} Â± {results['std_length']:.1f}")

        return results

    def save_model(self, filepath: str) -> None:
        """
        Salva modelo do agente.

        Args:
            filepath: Caminho para salvar o modelo
        """
        if self.policy and hasattr(self.policy, 'save'):
            self.policy.save(filepath)
            print(f"ğŸ’¾ Modelo do IndependentAgent {self.agent_id} salvo em {filepath}")

    def load_model(self, filepath: str) -> None:
        """
        Carrega modelo do agente.

        Args:
            filepath: Caminho do modelo a carregar
        """
        if self.policy and hasattr(self.policy, 'load'):
            self.policy.load(filepath)
            print(f"ğŸ“‚ Modelo do IndependentAgent {self.agent_id} carregado de {filepath}")

    def get_policy_info(self) -> Dict:
        """
        Retorna informaÃ§Ãµes da polÃ­tica.

        Returns:
            Dict: InformaÃ§Ãµes da polÃ­tica
        """
        if self.policy is None:
            return {"policy_type": None, "parameters": 0}

        info = {
            "policy_type": type(self.policy).__name__,
            "learning_rate": self.learning_rate,
            "exploration_rate": self.exploration_rate,
            "total_timesteps": self.total_timesteps
        }

        # Adicionar informaÃ§Ãµes especÃ­ficas do SB3 se disponÃ­veis
        if hasattr(self.policy, 'policy'):
            policy_net = self.policy.policy
            if hasattr(policy_net, 'parameters'):
                total_params = sum(p.numel() for p in policy_net.parameters())
                info["parameters"] = total_params

        return info

    def reset_exploration(self, exploration_rate: float = 1.0) -> None:
        """
        Reseta taxa de exploraÃ§Ã£o.

        Args:
            exploration_rate: Nova taxa de exploraÃ§Ã£o
        """
        self.exploration_rate = exploration_rate
        print(f"ğŸ”„ ExploraÃ§Ã£o do IndependentAgent {self.agent_id} resetada para {exploration_rate}")


class IndependentAgentFactory:
    """
    Factory especÃ­fico para agentes independentes.

    Permite configuraÃ§Ã£o otimizada para agentes independentes
    e facilita a criaÃ§Ã£o de mÃºltiplos agentes similares.
    """

    @staticmethod
    def create_agent(env, agent_id: int, config: Optional[Dict] = None) -> IndependentAgent:
        """
        Cria agente independente com configuraÃ§Ã£o padrÃ£o.

        Args:
            env: Ambiente de simulaÃ§Ã£o
            agent_id: ID do agente
            config: ConfiguraÃ§Ãµes especÃ­ficas (opcional)

        Returns:
            IndependentAgent: Agente configurado
        """
        default_config = {
            "agent_type": "independent",
            "learning_rate": 3e-4,
            "exploration_rate": 1.0,
            "min_exploration_rate": 0.1,
            "exploration_decay": 0.995,
            "n_steps": 2048,
            "batch_size": 64,
            "n_epochs": 10,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "clip_range": 0.2,
            "ent_coef": 0.01,
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
            "policy_kwargs": {
                "net_arch": [64, 64],
                "activation_fn": "Tanh"
            }
        }

        if config:
            default_config.update(config)

        return IndependentAgent(env, agent_id, default_config)

    @staticmethod
    def create_multi_agent_system(env, num_agents: Optional[int] = None,
                                config: Optional[Dict] = None) -> List[IndependentAgent]:
        """
        Cria sistema de mÃºltiplos agentes independentes.

        Args:
            env: Ambiente de simulaÃ§Ã£o
            num_agents: NÃºmero de agentes (se None, usa env.num_buildings)
            config: ConfiguraÃ§Ãµes base para todos os agentes

        Returns:
            List[IndependentAgent]: Lista de agentes independentes
        """
        if num_agents is None:
            num_agents = env.num_buildings

        agents = []
        for i in range(num_agents):
            agent_config = config.copy() if config else {}
            agent_config["agent_id"] = i

            agent = IndependentAgentFactory.create_agent(env, i, agent_config)
            agents.append(agent)

        print(f"âœ… Sistema de {len(agents)} agentes independentes criado")
        return agents